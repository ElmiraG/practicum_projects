# Метчинг товаров

Дано два множества объектов: множество A и множество B. Каждый объект в каждом множестве описывается определенными признаками. Цель состоит в том, чтобы для каждого объекта из множества A найти один или несколько объектов из множества B, которые схожи с ним по определенной метрике. Важно отметить, что множества A и B могут быть как различными, так и одинаковыми, и в процессе поиска соответствий может возникнуть ситуация, когда ни один объект из B не будет соответствовать объекту из A.

**Задача:**

- разработать алгоритм, который для всех товаров из validation.csv предложит несколько вариантов наиболее похожих товаров из base;

- оценить качество алгоритма по метрике accuracy@5

# План по выполнению проекта
1. Загрузка данных
2. Предобработка и исследовательский анализ данных
3. Подготовка выборок для обучения
4. Разработатка ранжирующих моделей
5. Анализ моделей
6. Отчет по исследованию

# Исходные данные

- **base.csv** - анонимизированный набор товаров. Каждый товар представлен как уникальный id (0-base, 1-base, 2-base) и вектор признаков размерностью 72.


- **train.csv** - обучающий датасет. Каждая строчка - один товар, для которого известен уникальный id (0-query, 1-query, …) , вектор признаков И id товара из base.csv, который максимально похож на него (по мнению экспертов).


- **validation.csv** - датасет с товарами (уникальный id и вектор признаков), для которых надо найти наиболее близкие товары из base.csv


- **validation_answer.csv** - правильные ответы к предыдущему файлу.

# Отчет по исследованию

1. Выводы в результате предобработки и анализа данных:

- Отсутствуют пропуски и дубликаты в данных
- Аномалии в данных не найдены
- Названия столбцов датафрейма переведены в соответствии с рекомендациями PEP 8
- В датасетах base, train и validation большинство столбцов имеют нормальное распределение, кроме столбцов 6, 21, 25, 33, 44, 59, 65, 70

2. Для поиска ближайших соседей использовалась библиотека FAISS.

   Для решения задачи созданы три выборки:
   - Первая выборка содержит исходные датасеты без изменений.
   - Вторая выборка включает только признаки с нормальным распределением, исключая столбцы: 6, 21, 25, 33, 44, 59, 65, 70.
   - Третья выборка содержит признаки из первых двух выборок, преобразованные с использованием метода главных компонент (PCA). Будет рассмотрено различное количество компонентов.

Для каждой выборки был произведен перебор размера кластера и количества ближайших кластеров.

Изначально планировалось использовать цикл для перебора наилучшего размера кластера. Однако расчёт проходил слишком медленно, поэтому были расмотрены всего 3 размера кластера: 50, 75 и 100. Увеличение размера кластера свыше 100 существенно увеличивало время расчёта. Также проведен перебор количества ближайших кластеров: 5, 10, 15, 20, 25.
Поиск ближайших соседей происходил только для первых 500 строк тренировочной выборки для снижения времени расчета.

3. После проведения расчета для первой выборки, были замечены следующие закономерности:

- С увеличением значения параметра n_cells (количество кластеров для индексирования) время работы модели снижается. Например, при увеличении n_cells с 50 до 100 время работы модели сокращается в среднем в 1.1 раз при n_nprobe равным 5.

Это может быть связано с тем, что увеличение количества кластеров позволяет более эффективно организовать данные для поиска ближайших соседей, что в свою очередь может ускорить процесс поиска. Но при этом, при увеличении параметра n_cells на своем ноутбуке свыше 100 - расчет задачи увеличивался на неопределенно длительное время.

- Увеличение значения n_nprobe обычно приводит к повышению точности модели, хотя некоторые изменения могут быть незначительными. Например, при увеличении n_nprobe с 5 до 25 точность увеличивается с 65.4% до 66.6% для n_cells равным 50. При увеличении параметра n_nprobe увеличивается и время расчета. Например:
    -  при увеличении n_nprobe с 5 до 25 время работы модели увеличивается в 3.2 раз при n_cells равным 50.
    -  при увеличении n_nprobe с 5 до 25 время работы модели увеличивается в 2.7 раз при n_cells равным 75.
    -  при увеличении n_nprobe с 5 до 25 время работы модели увеличивается в 2.5 раз при n_cells равным 100.
   
Это может свидетельствовать о том, что при более низких значениях n_cells увеличение количества ближайших кластеров n_nprobe оказывает более существенное влияние на производительность модели, приводя к увеличению времени работы в большей степени, чем при более высоких значениях n_cells.

4. Для первой выборки максимальное accuracy равно 66.6, достигается при n_cells = 50 и при n_nprobe равным 20 и 25. При n_nprobe = 20, расчет занимает в 1.2 раза меньше времени
5. В результате расчета над второй выборкой были сделаны следующие выводы:
   
   - Максимальное accuracy равно 70.8, достигается при n_cells = 50 и при n_nprobe равным 15, 20 и 25. Минимальное время для данного значения кластеров, достигается при n_nprobe = 15

   - Анализ признаков и последующее удаление признаков с ненормальным распределением увеличило метрику accuracy с 66.6 до 70.8
  
6. При изучении Explained Variance для двух выше перечисленных выборок, были сделаны выводы:

- Для первой выборки:
     - 40 компонентов объясняет чуть больше 60% данных
     - 50 компонентов - чуть меньше 80%
- Для второй выборки:
     - 40 компонентов объясняет 70% данных
     - 50 компонентов - чуть больше 80%

График Explained Variance, который двигается практически пропорционально увеличению количества компонентов после применения PCA, говорит о том, что каждая дополнительная компонента объясняет дополнительную дисперсию в данных. То есть, принаки к датасетах достаточно равнозначны. Добавление большего количества компонентов помогает сохранить больше информации из исходных данных.

7. Лучший результат для данных проведенных через PCA() - достигнут с accuracy=70.4 при снижении дисперсии примерно на 20% с количеством компонент равным 50 для выборки, в которой были удалены признаки с ненормальным распределением.
Параметры: n_cells = 50, n_nprobe = 25. Время работы модели с такими же параметрами снизилась в 1.4 раза, а метрика снизилась на 0.4%.При сравнении с лучшей моделью из второй выборки с параметрами n_cells = 50, n_nprobe = 15, метрика упала с 70.8% до 69.8%, время работы снизилась в 2 раза.

8. По итогу, лучше всего показали себя модели из второй выборки и модели из той же выборки, но проведенные через PCA() со снижением дисперсии на 20%.
Наиболее высокий показатель accuracy = 70.8% с оптимальным количеством времени обучения - достигнут для второй выборки с n_cells = 50 и n_nprobe = 15.

 Получается, что лучший результат достигается при изначальной правильной обработке данных. И только потом на время обучения и метрику может повлиять правильный набор гиперпараметров. Использование PCA() для обработки данных может снизить длительность расчета при незначительном снижении метрики - с разницей в десятых долях, - если использовать для анализа несколько моделей с разным количеством параметров.
